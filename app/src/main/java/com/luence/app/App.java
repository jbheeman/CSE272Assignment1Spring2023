/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.luence.app;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.*;
import org.apache.lucene.store.ByteBuffersDirectory;
import org.apache.lucene.store.Directory;
import org.apache.lucene.util.BytesRef;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;

enum AlgorithmTypes {
    BOOLEAN,
    TF,
    TFIDF,
    PSEUDO_RELEVANCE,
    CUSTOM_QUERY_EXPANSION;

    String getString() {
        switch (this) {
            case TF -> {
                return "tf_model";
            }
            case TFIDF ->  {
                return "tfidf_model";
            }
            case BOOLEAN -> {
                return  "boolean_model";
            }
            case PSEUDO_RELEVANCE ->  {
                return "pseudo_relevance_model";
            }
            case CUSTOM_QUERY_EXPANSION -> {
                return "query_expansion_model";
            }
            default ->  {
                return "";
            }
        }
    }
}
class QueryObject {

    String num, title, desc;

    public QueryObject(String num, String title, String desc) {
        this.num = num;
        this.title = title;
        this.desc = desc;
    }

    String clean_title() {
        String [] str_words = this.title.replace(".", "")
                .toLowerCase().replace(" yo ", " year old ")
                .replace("s/p", "with")
                .replace("y. o. w. f.", "year old white female")
                .replace("w/", "with")
                .replace("h/o", "").replace(" yr ", "year").replace("/", " ").split(" ");
        StringBuilder new_str_builder = new StringBuilder();
        for(String w : str_words) {
            if(!StopWords.stop_words.contains(w)) {
                new_str_builder.append(w).append(" ");
            }
        }
        return new_str_builder.toString();

    }

    String clean_description() {
        String [] str_words = this.desc.replace(".", "")
                .toLowerCase().replace(" yo ", " year old ")
                .replace("s/p", "with")
                .replace("y. o. w. f.", "year old white female")
                .replace("w/", "with")
                .replace("h/o", "").replace(" yr ", "year").replace("/", " ").split(" ");

        StringBuilder new_str_builder = new StringBuilder();
        for(String w : str_words) {
            if(!StopWords.stop_words.contains(w)) {
                new_str_builder.append(w).append(" ");
            }
        }
        return new_str_builder.toString();
    }
}
class DocumentObject {

    int index, document_id;
    String s_type, m_type, title, p_type, document_text, author;
    public DocumentObject(int index, int document_id, String s_type, String m_type, String title, String p_type, String document_text, String author) {
        this.index = index;
        this.s_type = s_type;
        this.m_type = m_type;
        this.title = title;
        this.p_type = p_type;
        this.document_id = document_id;
        this.document_text = document_text;
        this.author = author;
    }

    String clean_s_type() {
        String[] arr = this.s_type.split("\s");
        return arr[0] + " " + arr[1];
    }

    String clean_m_type() {
        String[] arr = this.m_type.split(";");
        String final_result = "";
        for (String a : arr) {
            final_result += ";" + a.split("/")[0];
        }
        return final_result;
    }

    String clean_p_type() {
        return this.p_type.replace(".", "");
    }

    String clean_title() {
        return this.title.replace(".", "").toLowerCase().replace(" yo ", " year old ").replace("s/p", "with").replace("y. o. w. f.", "year old white female");
    }

    String clean_doc() {
        return this.document_text.replace(".", "").toLowerCase().replace(" yo ", " year old ").replace("s/p", "with").replace("y. o. w. f.", "year old white female");
    }

    String clean_author() {
        return this.author.replace(".", "");
    }
}



public class App {

    // https://stackoverflow.com/questions/43414751/is-it-possible-to-find-common-words-in-specific-lucene-documents
    private static String getTopTerm(int docId, String field, IndexReader reader) throws IOException {

        Terms terms = reader.getTermVector(docId, field);
        TermsEnum it = terms.iterator();
        String max_term = "";
        long max_freq = 0;
        for(BytesRef br = it.next(); br != null; br = it.next()) {
            if(it.totalTermFreq() > max_freq) {
                max_term = br.utf8ToString();
                max_freq = it.totalTermFreq();
            }

        }
        return max_term;
    }

    private static void addDoc(IndexWriter w, DocumentObject d) throws IOException {
        // https://stackoverflow.com/questions/43414751/is-it-possible-to-find-common-words-in-specific-lucene-documents
        FieldType type = new FieldType();
        type.setTokenized(true);
        type.setStoreTermVectors(true);
        type.setStored(false);
        type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
        Document doc = new Document();
        doc.add(new StringField("document_id", String.valueOf(d.document_id), Field.Store.YES));
        doc.add(new StringField("s_type", d.clean_s_type(), Field.Store.YES));
        doc.add(new TextField("m_type", d.clean_m_type(), Field.Store.YES));
        doc.add(new StringField("p_type", d.clean_p_type(), Field.Store.YES));
        doc.add(new TextField("title", d.clean_title(), Field.Store.YES));
        doc.add(new TextField("document", d.clean_doc(), Field.Store.YES));
        doc.add(new TextField("author", d.clean_author(), Field.Store.YES));
        doc.add(new Field("common_words_content", d.clean_title() + " " + d.clean_doc(), type));
        w.addDocument(doc);
    }


    static private ArrayList<QueryObject> parseQueries(String path) throws FileNotFoundException {
        File f = new File(path);
        Scanner scan = new Scanner(f);
        ArrayList<QueryObject> queries = new ArrayList<>();
        while (scan.hasNextLine()) {
            String data = scan.nextLine();
            QueryObject q = new QueryObject("", "", "");
            if(data.startsWith("<top>")) {
                data = scan.nextLine();
            }
            if (data.startsWith("<num>")) {
                q.num = data.split(":\s")[1].replace("\n", "");
                data = scan.nextLine();
            }
            if (data.startsWith("<title>")) {
                q.title = data.split("<title> ")[1].replace("\n", "");
                data = scan.nextLine();
            }

            if (data.startsWith("<desc>")) {
                data = scan.nextLine();
                q.desc = data.replace("\n", "");
                data = scan.nextLine();
                data = scan.nextLine();
            }
            queries.add(q);
        }
        return queries;
    }

    static private ArrayList<DocumentObject> parseDocuments(String path) throws FileNotFoundException {
        File file = new File(path);
        Scanner scan = new Scanner(file);
        ArrayList<DocumentObject> documents = new ArrayList<>();
        while (scan.hasNextLine()) {
            String data = scan.nextLine();
            int index_id = -1, document_id = -1;

            DocumentObject doc = new DocumentObject(-1, -1, "", "", "", "", "", "");
            if(data.startsWith(".I")) {
                index_id = Integer.parseInt(data.split("\s")[1].replace("\n", ""));
                data = scan.nextLine();
                doc.index = index_id;
            }

            if (data.startsWith(".U")) {
                data = scan.nextLine().replace("\n", "");
                doc.document_id = Integer.parseInt(data);
                data = scan.nextLine();
            }

            if (data.startsWith(".S")) {
                data = scan.nextLine().replace("\n", "");
                doc.s_type = String.valueOf(data);
                data = scan.nextLine();
            }

            if (data.startsWith(".M")) {
                data = scan.nextLine().replace("\n", "");
                doc.m_type = String.valueOf(data);
                data = scan.nextLine();
            }

            if(data.startsWith(".T")) {
                data = scan.nextLine().replace("\n", "");
                doc.title = String.valueOf(data);
                data = scan.nextLine();
            }

            if(data.startsWith(".P")) {
                data = scan.nextLine().replace("\n", "");
                doc.p_type = String.valueOf(data);
                data = scan.nextLine();
            }

            if(data.startsWith(".W")) {
                data = scan.nextLine().replace("\n", "");
                doc.document_text = String.valueOf(data);
                data = scan.nextLine();
            }

            if(data.startsWith(".A")) {
                data = scan.nextLine().replace("\n", "");
                doc.author = String.valueOf(data);
            }
            documents.add(doc);
        }
        scan.close();
        return documents;

    }

    public static void main(String[] args) throws IOException, ParseException {



        AlgorithmTypes algorithm = AlgorithmTypes.BOOLEAN;


        Path root_path = Paths.get("/Users/jeshbheemanpally/Desktop/CSE272/CSE272_UCSC_Spring/HW1/java/app/src/main/java/com/luence/app/data/");
        ArrayList<DocumentObject> docs = App.parseDocuments(Paths.get(root_path.toString(),"ohsumed.88-91").toString());

        ArrayList<QueryObject> queryObj = App.parseQueries(Paths.get(root_path.toString(),"query.ohsu.1-63").toString());


        StandardAnalyzer analyzer = new StandardAnalyzer();
        Directory index = new ByteBuffersDirectory();


        IndexWriterConfig config = new IndexWriterConfig(analyzer);

        try (IndexWriter w = new IndexWriter(index, config)) {

            for (DocumentObject doc : docs) {

                addDoc(w, doc);
            }

        }


        IndexReader reader = DirectoryReader.open(index);
        IndexSearcher searcher = new IndexSearcher(reader);



        if (algorithm == AlgorithmTypes.TF) {
            searcher.setSimilarity(new TFSimilarity());
            config.setSimilarity(new TFSimilarity());
        } else if (algorithm == AlgorithmTypes.TFIDF) {
            config.setSimilarity(new TFIDFSimilarity());
            searcher.setSimilarity(new TFIDFSimilarity());
        }


        String file_path = "./" + algorithm.getString() + "_result.txt";
        File result_file = new File(file_path);
        result_file.delete();
        result_file.createNewFile();

        FileWriter writer = new FileWriter(file_path);


        for(int queryIndex = 0; queryIndex < queryObj.size(); queryIndex++) {
            QueryObject queryObject = queryObj.get(queryIndex);

            String query_str = queryObject.clean_title() + "\s" + queryObject.clean_description();
            String [] title_words = query_str.split("\s");

            String [] description_words = queryObject.clean_description().split(",");

            StringBuilder new_query = new StringBuilder();

            BooleanQuery.Builder bool_query = new BooleanQuery.Builder();

           ArrayList<String> word_set = new ArrayList<>();

            // for title query
            for(String w: title_words) {
                if (!StopWords.stop_words.contains(w)) {
                    if(w.length() > 3) {
                        if(!word_set.contains(w)) {
                            word_set.add(w);
                        }
                    }
                    bool_query.add(new TermQuery(new Term("title", w)), BooleanClause.Occur.SHOULD);
                    bool_query.add(new TermQuery(new Term("document", w)), BooleanClause.Occur.SHOULD);
                }
            }

            // for description query
            for(String w: description_words) {
                if (!StopWords.stop_words.contains(w)) {
                    bool_query.add(new TermQuery(new Term("title", w)), BooleanClause.Occur.SHOULD);
                    bool_query.add(new TermQuery(new Term("document", w)), BooleanClause.Occur.SHOULD);
                }
            }

            if(algorithm == AlgorithmTypes.CUSTOM_QUERY_EXPANSION) {
                for(String w: word_set) {
                    String syn = APISynonym.getSynonym(w);
                    if (syn.contains(",")) {
                        syn = syn.split(",")[0];
                        query_str += " " + syn;
                    }
                }
            }


            Query q = new QueryParser("title", analyzer).parse(query_str);



            int hitsPerPage = 50;

            TopDocs top_doc = searcher.search(q, hitsPerPage);
            ScoreDoc[] hits = top_doc.scoreDocs;

            int top_k_hits = hits.length;
            if (top_k_hits > 10) {
                top_k_hits = 10;
            }

            if(algorithm == AlgorithmTypes.PSEUDO_RELEVANCE) {
                StringBuilder expanded_query = new StringBuilder(query_str);
                for (int i = 0; i < top_k_hits; ++i) {
                    int docId = hits[i].doc;
                    reader.document(docId).getField("title").stringValue();
                    float score_val = hits[i].score;
                    String top_term = getTopTerm(docId, "common_words_content", reader);
                    expanded_query.append(" ").append(top_term);
                }
                q = new QueryParser("title", analyzer).parse(expanded_query.toString());

                top_doc = searcher.search(q, hitsPerPage);
                hits = top_doc.scoreDocs;
            }

            for (int i = 0; i < hits.length; ++i) {
                int docId = hits[i].doc;
                float score_val = hits[i].score;
                Document d = searcher.getIndexReader().document(docId);
                String document_id = d.getField("document_id").stringValue();
                writer.write(queryObject.num + " " + "Q" + queryIndex + " " + document_id + " " + (i + 1) + " " + score_val + " " + algorithm.getString() + "\n");
            }
        }
        writer.close();


    }
}
